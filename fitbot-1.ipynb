{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "r7j5dHqk7m3v"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\asus\\anaconda3\\lib\\site-packages (4.45.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\asus\\anaconda3\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from transformers) (0.25.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from transformers) (1.24.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in c:\\users\\asus\\anaconda3\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from transformers) (0.20.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from requests->transformers) (2024.8.30)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "80ndW4rs7qt6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspellchecker in c:\\users\\asus\\anaconda3\\lib\\site-packages (0.8.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspellchecker\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### This is the file for Fine tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, TextDataset, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
    "\n",
    "# Step 1: Load and clean the dataset\n",
    "def cleaning(s):\n",
    "    s = str(s)\n",
    "    s = re.sub('\\s\\W',' ',s)\n",
    "    s = re.sub('\\W,\\s',' ',s)\n",
    "    s = re.sub(\"\\d+\", \"\", s)\n",
    "    s = re.sub('\\s+',' ',s)\n",
    "    s = re.sub('[!@#$_]', '', s)\n",
    "    return s\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv('/content/cleaned_fitness_qa_dataset.csv')\n",
    "# Update the column names in the cleaning step\n",
    "df['question'] = df['question'].apply(cleaning)\n",
    "df['answer'] = df['answer'].apply(cleaning)\n",
    "\n",
    "# Combine Question and Answer for training\n",
    "df['QA'] = \"Question: \" + df['question'] + \" Answer: \" + df['answer']\n",
    "\n",
    "# Save the cleaned dataset to a text file\n",
    "train_file_path = \"/content/fitness_qa_train.txt\"\n",
    "with open(train_file_path, 'w') as f:\n",
    "    f.write(\"\\n\".join(df['QA']))\n",
    "\n",
    "\n",
    "# Step 2: Define functions for loading dataset and fine-tuning GPT-2\n",
    "def load_dataset(file_path, tokenizer, block_size=128):\n",
    "    return TextDataset(\n",
    "        tokenizer=tokenizer,\n",
    "        file_path=file_path,\n",
    "        block_size=block_size\n",
    "    )\n",
    "\n",
    "def load_data_collator(tokenizer, mlm=False):\n",
    "    return DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer,\n",
    "        mlm=mlm\n",
    "    )\n",
    "\n",
    "# Step 3: Fine-tune GPT-2\n",
    "model_name = 'gpt2'\n",
    "output_dir = '/content/fitness_gpt2_model'\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token  # GPT-2 requires a padding token\n",
    "\n",
    "# Load dataset and collator\n",
    "train_dataset = load_dataset(train_file_path, tokenizer)\n",
    "data_collator = load_data_collator(tokenizer)\n",
    "\n",
    "# Load GPT-2 model\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=8,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    logging_dir='/content/logs',\n",
    "    logging_steps=100,\n",
    "    prediction_loss_only=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "trainer.save_model(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "# Step 4: Define text generation function\n",
    "def generate_text(sequence, max_length, model_path, tokenizer_path):\n",
    "    model = GPT2LMHeadModel.from_pretrained(model_path)\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(tokenizer_path)\n",
    "    ids = tokenizer.encode(f'{sequence}', return_tensors='pt')\n",
    "    outputs = model.generate(\n",
    "        ids,\n",
    "        do_sample=True,\n",
    "        max_length=max_length,\n",
    "        top_k=50,\n",
    "        top_p=0.95,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Test the model\n",
    "model_path = output_dir\n",
    "tokenizer_path = output_dir\n",
    "sequence = \"How do I build muscle?\"\n",
    "max_length = 50\n",
    "response = generate_text(sequence, max_length, model_path, tokenizer_path)\n",
    "print(\"Generated Response:\", response)\n",
    "###"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "import shutil\n",
    "\n",
    "# Define the directory where the trained model is saved\n",
    "output_dir = '/content/fitness_gpt2_model'\n",
    "\n",
    "# Define the zip file path\n",
    "zip_file_path = '/content/fitness_gpt2_model.zip'\n",
    "\n",
    "# Create a zip archive of the trained model directory\n",
    "shutil.make_archive(base_name=zip_file_path.replace('.zip', ''), format='zip', root_dir=output_dir)\n",
    "\n",
    "# The zip file is now saved at '/content/fitness_gpt2_model.zip'\n",
    "print(f\"Model zipped at: {zip_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "NjWQb4FE7jPd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#import statements\n",
    "import pandas as pd\n",
    "import json\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "import requests\n",
    "from spacy.pipeline import EntityRuler\n",
    "from typing import List, Dict, Tuple\n",
    "import time\n",
    "import spacy\n",
    "from spellchecker import SpellChecker\n",
    "import re\n",
    "import logging\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "# Download NLTK stop words (if not already downloaded)\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "logging.getLogger(\"transformers\").setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "65tHNoHH64gl",
    "outputId": "8a5af282-9ae9-45c1-dd54-02af04f4c599"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L6e08uHm64u7"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zfw2S8SGiTUH"
   },
   "source": [
    "# **NER for `FOOD`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fmFeQoEV6_Zh"
   },
   "source": [
    "**NER using Entity ruler for the identification of food items**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "yxrplrzEABMX"
   },
   "outputs": [],
   "source": [
    "#Adding food label to the following items in the Entity Ruler\n",
    "def add_fooditems(nlp):\n",
    "\n",
    "    # Add custom food entities to the pipeline\n",
    "    food_items = [\n",
    "    # Meats and Fish\n",
    "    \"Chicken\", \"Mutton\", \"Pork\", \"Chicken Breast\", \"Chicken Thigh\", \"Chicken Wings\", \"Turkey Breast\",\n",
    "    \"Ground Turkey\", \"Beef\", \"Lean Beef\", \"Ground Beef\", \"Beef Steak\", \"Pork Chop\", \"Pork Tenderloin\",\n",
    "    \"Bacon\", \"Ham\", \"Salmon Fillet\", \"Tuna\", \"Tilapia\", \"Cod\", \"Shrimp\", \"Sardines\", \"Lamb Chop\",\n",
    "\n",
    "    # Eggs and Dairy\n",
    "    \"Whole Egg\", \"Egg White\", \"Whole Milk\", \"Skim Milk\", \"Greek Yogurt\", \"Regular Yogurt\", \"Cottage Cheese\",\n",
    "    \"Cheddar Cheese\", \"Mozzarella Cheese\", \"String Cheese\", \"Whipped Cream\", \"Butter\",\n",
    "\n",
    "    # Plant Based Proteins\n",
    "    \"Tofu\", \"Tempeh\", \"Seitan\", \"Black Beans\", \"Kidney Beans\", \"Chickpeas\", \"Lentils\", \"Edamame\", \"Black Eyed Peas\",\n",
    "    \"Quinoa\",\n",
    "\n",
    "    # Grains and Starches\n",
    "    \"White Rice\", \"Brown Rice\", \"Jasmine Rice\", \"White Bread\", \"Whole Wheat Bread\", \"Sourdough Bread\", \"Bagel\",\n",
    "    \"English Muffin\", \"Tortilla\", \"Pita Bread\", \"Oatmeal\", \"Steel Cut Oats\", \"Cream of Wheat\", \"Pasta\",\n",
    "    \"Whole Wheat Pasta\", \"Sweet Potato\", \"White Potato\", \"Corn\",\n",
    "\n",
    "    # Vegetables\n",
    "    \"Broccoli\", \"Spinach\", \"Kale\", \"Lettuce\", \"Cucumber\", \"Tomato\", \"Bell Pepper\", \"Carrots\", \"Green Beans\",\n",
    "    \"Asparagus\", \"Brussels Sprouts\", \"Cauliflower\", \"Zucchini\", \"Eggplant\", \"Mushrooms\", \"Onion\", \"Garlic\", \"Celery\",\n",
    "    \"Beets\", \"Artichoke\",\n",
    "\n",
    "    # Fruits\n",
    "    \"Apple\", \"Banana\", \"Orange\", \"Strawberries\", \"Blueberries\", \"Raspberries\", \"Blackberries\", \"Grapes\",\n",
    "    \"Pineapple\", \"Mango\", \"Pear\", \"Peach\", \"Plum\", \"Watermelon\", \"Cantaloupe\", \"Honeydew\", \"Kiwi\", \"Pomegranate\",\n",
    "    \"Grapefruit\", \"Lemon\", \"Lime\",\n",
    "\n",
    "    # Nuts and Seeds\n",
    "    \"Almonds\", \"Walnuts\", \"Cashews\", \"Pistachios\", \"Pecans\", \"Brazil Nuts\", \"Macadamia Nuts\", \"Peanuts\",\n",
    "    \"Chia Seeds\", \"Flax Seeds\", \"Sunflower Seeds\", \"Pumpkin Seeds\", \"Hemp Seeds\",\n",
    "\n",
    "    # Healthy Fats\n",
    "    \"Avocado\", \"Olive Oil\", \"Coconut Oil\", \"Avocado Oil\", \"Peanut Butter\", \"Almond Butter\", \"Cashew Butter\", \"Tahini\",\n",
    "\n",
    "    # Supplements\n",
    "    \"Whey Protein\", \"Casein Protein\", \"Plant Protein\", \"Mass Gainer\", \"Protein Bar\", \"Energy Bar\", \"Pre Workout\",\n",
    "    \"Creatine\", \"BCAA\",\n",
    "\n",
    "    # Processed and Packaged Foods\n",
    "    \"Protein Cookie\", \"Protein Chips\", \"Rice Cakes\", \"Crackers\", \"Granola\", \"Cereal\", \"Protein Cereal\", \"Trail Mix\",\n",
    "    \"Dried Fruit\", \"Beef Jerky\", \"Turkey Jerky\", \"Protein Shake\", \"Smoothie\",\n",
    "\n",
    "    # Condiments and Sauces\n",
    "    \"Hummus\", \"Mustard\", \"Hot Sauce\", \"Salsa\", \"Guacamole\", \"Greek Yogurt Dip\", \"Protein Spread\", \"Sugar Free Syrup\",\n",
    "    \"Low Fat Dressing\",\n",
    "\n",
    "    # Beverages\n",
    "    \"Black Coffee\", \"Green Tea\", \"Black Tea\", \"Protein Coffee\", \"Sports Drink\", \"Energy Drink\", \"Coconut Water\",\n",
    "    \"Almond Milk\", \"Soy Milk\", \"Oat Milk\", \"Protein Water\"\n",
    "]\n",
    "    ruler = nlp.add_pipe(\"entity_ruler\", before=\"ner\")\n",
    "    patterns = [{\"label\": \"FOOD\", \"pattern\": item.lower()} for item in food_items]\n",
    "    ruler.add_patterns(patterns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "E2u1F87fBDFg"
   },
   "outputs": [],
   "source": [
    "#method to get the nutrition\n",
    "def get_calorie(items):\n",
    "    api_url = 'https://api.calorieninjas.com/v1/nutrition?query='\n",
    "    for item in items:\n",
    "        query = item\n",
    "        response = requests.get(api_url + query, headers={'X-Api-Key': '41XzPlIpAcWTDGPLoj6WAA==NcwVJXSV0o0BEeJ3'})\n",
    "        if response.status_code == requests.codes.ok:\n",
    "            return response.json()\n",
    "        else:\n",
    "            return f\"Error: {response.status_code}, {response.text}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "6DoaFF87_-DR"
   },
   "outputs": [],
   "source": [
    "#modifying the information obtained by the nutrition API in a readable format\n",
    "def print_nutritional_info(data):\n",
    "    for item in data[\"items\"]:\n",
    "        paragraph = f\"\"\"The item is a {item['name']}. A 100g serving contains: {item['calories']} calories, {item['fat_total_g']}g of total fat, including {item['fat_saturated_g']}g of saturated fat, {item['protein_g']}g of protein {item['sodium_mg']}mg of sodium, {item['potassium_mg']}mg of potassium, {item['cholesterol_mg']}mg of cholesterol, {item['carbohydrates_total_g']}g of total carbohydrates, with {item['fiber_g']}g of fiber and {item['sugar_g']}g of sugar\"\"\"\n",
    "    return paragraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NbMm0GMqhYqQ"
   },
   "source": [
    "**Main method to identify the food items and get the nutrition information**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "552HppMeBh6S"
   },
   "outputs": [],
   "source": [
    "def nutrition(nlp,text):\n",
    "    #NER identification\\\n",
    "    doc = nlp(text.lower())\n",
    "    items = [ent.text for ent in doc.ents if ent.label_ == \"FOOD\"]\n",
    "    if(items):\n",
    "        data = get_calorie(items)\n",
    "        if(data):\n",
    "            return(print_nutritional_info(data))\n",
    "        else:\n",
    "            return None\n",
    "    else :\n",
    "        return None\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DZc2FrjBie0A"
   },
   "source": [
    "# **NER FOR `BODY_PART`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iiB_30fyi-P3"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "AeaHwyQMh0Py"
   },
   "outputs": [],
   "source": [
    "#NER MODEL using Entity Ruler method\n",
    "\n",
    "def create_fitness_body_parts_ner(patterns: List[Dict[str, str]] = None) -> spacy.language.Language:\n",
    "    \"\"\"\n",
    "    Create a custom NER pipeline for detecting fitness-related body parts.\n",
    "\n",
    "    Args:\n",
    "        patterns: Optional list of pattern dictionaries to override default patterns\n",
    "\n",
    "    Returns:\n",
    "        nlp: Configured spaCy pipeline with fitness body parts EntityRuler\n",
    "    \"\"\"\n",
    "    nlp = spacy.blank(\"en\")\n",
    "\n",
    "    if patterns is None:\n",
    "        patterns = [\n",
    "    {\"label\": \"BODY_PART\", \"pattern\": \"biceps\"},\n",
    "    {\"label\": \"BODY_PART\", \"pattern\": \"triceps\"},\n",
    "    {\"label\": \"BODY_PART\", \"pattern\": \"deltoids\"},\n",
    "    {\"label\": \"BODY_PART\", \"pattern\": \"delts\"},\n",
    "    {\"label\": \"BODY_PART\", \"pattern\": \"shoulders\"},\n",
    "    {\"label\": \"BODY_PART\", \"pattern\": \"pectorals\"},\n",
    "    {\"label\": \"BODY_PART\", \"pattern\": \"pecs\"},\n",
    "    {\"label\": \"BODY_PART\", \"pattern\": \"chest\"},\n",
    "    {\"label\": \"BODY_PART\", \"pattern\": \"upper chest\"},\n",
    "    {\"label\": \"BODY_PART\", \"pattern\": \"lower chest\"},\n",
    "    {\"label\": \"BODY_PART\", \"pattern\": \"abs\"},\n",
    "    {\"label\": \"BODY_PART\", \"pattern\": \"abdominals\"},\n",
    "    {\"label\": \"BODY_PART\", \"pattern\": \"upper abs\"},\n",
    "    {\"label\": \"BODY_PART\", \"pattern\": \"lower abs\"},\n",
    "    {\"label\": \"BODY_PART\", \"pattern\": \"obliques\"},\n",
    "    {\"label\": \"BODY_PART\", \"pattern\": \"core\"},\n",
    "    {\"label\": \"BODY_PART\", \"pattern\": \"quadriceps\"},\n",
    "    {\"label\": \"BODY_PART\", \"pattern\": \"quads\"},\n",
    "    {\"label\": \"BODY_PART\", \"pattern\": \"hamstrings\"},\n",
    "    {\"label\": \"BODY_PART\", \"pattern\": \"calves\"},\n",
    "    {\"label\": \"BODY_PART\", \"pattern\": \"glutes\"},\n",
    "    {\"label\": \"BODY_PART\", \"pattern\": \"lats\"},\n",
    "    {\"label\": \"BODY_PART\", \"pattern\": \"traps\"},\n",
    "    {\"label\": \"BODY_PART\", \"pattern\": \"back\"},\n",
    "    {\"label\": \"BODY_PART\", \"pattern\": \"upper back\"},\n",
    "    {\"label\": \"BODY_PART\", \"pattern\": \"lower back\"},\n",
    "    {\"label\": \"BODY_PART\", \"pattern\": \"mid back\"},\n",
    "    {\"label\": \"BODY_PART\", \"pattern\": \"neck\"},\n",
    "    {\"label\": \"BODY_PART\", \"pattern\": \"arms\"},\n",
    "    {\"label\": \"BODY_PART\", \"pattern\": \"forearms\"},\n",
    "    {\"label\": \"BODY_PART\", \"pattern\": \"legs\"},\n",
    "    {\"label\": \"BODY_PART\", \"pattern\": \"thighs\"},\n",
    "    {\"label\": \"BODY_PART\", \"pattern\": \"knees\"},\n",
    "    {\"label\": \"BODY_PART\", \"pattern\": \"ankles\"},\n",
    "    {\"label\": \"BODY_PART\", \"pattern\": \"upper body\"},\n",
    "    {\"label\": \"BODY_PART\", \"pattern\": \"lower body\"},\n",
    "    {\"label\": \"BODY_PART\", \"pattern\": \"midsection\"},\n",
    "    {\"label\": \"BODY_PART\", \"pattern\": \"waist\"},\n",
    "    {\"label\": \"BODY_PART\", \"pattern\": \"hips\"},\n",
    "    {\"label\": \"BODY_PART\", \"pattern\": \"shoulder\"},\n",
    "    {\"label\": \"BODY_PART\", \"pattern\": \"trapezius\"},\n",
    "    {\"label\": \"BODY_PART\", \"pattern\": \"inner thighs\"},\n",
    "    {\"label\": \"BODY_PART\", \"pattern\": \"outer thighs\"},\n",
    "    {\"label\": \"BODY_PART\", \"pattern\": \"hands\"},\n",
    "    {\"label\": \"BODY_PART\", \"pattern\": \"fingers\"},\n",
    "    {\"label\": \"BODY_PART\", \"pattern\": \"wrists\"},\n",
    "    {\"label\": \"BODY_PART\", \"pattern\": \"thumbs\"},\n",
    "    {\"label\": \"BODY_PART\", \"pattern\": \"feet\"},\n",
    "    {\"label\": \"BODY_PART\", \"pattern\": \"toes\"},\n",
    "    {\"label\": \"BODY_PART\", \"pattern\": \"elbows\"},\n",
    "    {\"label\": \"BODY_PART\", \"pattern\": \"shoulder joints\"},\n",
    "    {\"label\": \"BODY_PART\", \"pattern\": \"hip joints\"},\n",
    "    {\"label\": \"BODY_PART\", \"pattern\": \"wrist joints\"},\n",
    "    {\"label\": \"BODY_PART\", \"pattern\": \"calf muscles\"},\n",
    "    {\"label\": \"BODY_PART\", \"pattern\": \"forearm muscles\"},\n",
    "    {\"label\": \"BODY_PART\", \"pattern\": \"lower legs\"},\n",
    "    {\"label\": \"BODY_PART\", \"pattern\": \"upper legs\"},\n",
    "]\n",
    "\n",
    "\n",
    "    # Create the EntityRuler and add it to the pipeline\n",
    "    ruler = nlp.add_pipe(\"entity_ruler\", config={\"overwrite_ents\": True})\n",
    "\n",
    "    # Add the patterns to the ruler\n",
    "    ruler.add_patterns(patterns)\n",
    "\n",
    "    return nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "o-wBJWSGinXI"
   },
   "outputs": [],
   "source": [
    "#method to identify the BODY_PARTS\n",
    "def extract_body_parts(text: str, nlp: spacy.language.Language) -> List[Tuple[str, int, int]]:\n",
    "    \"\"\"\n",
    "    Extract fitness-related body parts from text.\n",
    "\n",
    "    Args:\n",
    "        text: Input text to process\n",
    "        nlp: Configured spaCy pipeline with body parts EntityRuler\n",
    "\n",
    "    Returns:\n",
    "        List of tuples containing (body_part, start_char, end_char)\n",
    "    \"\"\"\n",
    "    doc = nlp(text)\n",
    "    return [(ent.text, ent.start_char, ent.end_char) for ent in doc.ents if ent.label_ == \"BODY_PART\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "AdubtzTgi8RA"
   },
   "outputs": [],
   "source": [
    "# Initialize the spell checker\n",
    "spell = SpellChecker()\n",
    "\n",
    "def autocorrect_sentence(sentence):\n",
    "    # Tokenize the sentence into words\n",
    "    words = sentence.split()\n",
    "\n",
    "    # Correct each word\n",
    "    corrected_words = []\n",
    "    for word in words:\n",
    "        if word.lower() in spell:  # Check if the word is valid\n",
    "            corrected_words.append(word)  # Keep the word as is\n",
    "        else:\n",
    "            corrected_word = spell.correction(word)  # Suggest a correction\n",
    "            corrected_words.append(corrected_word or word)  # Use correction if available\n",
    "\n",
    "    # Join the corrected words back into a sentence\n",
    "    corrected_sentence = ' '.join(corrected_words)\n",
    "    return corrected_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "HTzh0_vaireA"
   },
   "outputs": [],
   "source": [
    "  def get_muscle_id():\n",
    "      url = \"https://wger.de/api/v2/muscle/\"\n",
    "\n",
    "      # Fetch data from the API\n",
    "      response = requests.get(url)\n",
    "\n",
    "      # Check if the request was successful\n",
    "      if response.status_code == 200:\n",
    "          data = response.json()\n",
    "          muscles = data[\"results\"]\n",
    "          muscle_dict = {muscle[\"name_en\"]: muscle[\"id\"] for muscle in muscles}\n",
    "          return muscle_dict\n",
    "      else:\n",
    "          print(\"Failed to retrieve data:\", response.status_code, response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "_o6ClTmzeMnH"
   },
   "outputs": [],
   "source": [
    "# Predefined mapping of body part names to muscle IDs\n",
    "muscle_name_to_id = {\n",
    "    \"chest\": 4,\n",
    "    \"biceps\": 1,\n",
    "    \"arms\":1,\n",
    "    \"triceps\": 5,\n",
    "    \"legs\": 8,\n",
    "    \"back\": 12,\n",
    "    \"shoulders\": 2,\n",
    "    \"abs\": 6,\n",
    "    \"calves\": 7,\n",
    "    'Quads': 10,\n",
    "    'Lats': 12,\n",
    "    'Hamstrings': 11,\n",
    "    'Glutes': 8,\n",
    "}\n",
    "\n",
    "def get_exercises_by_body_part(body_part_name):\n",
    "    \"\"\"\n",
    "    Fetch exercises based on a body part name.\n",
    "\n",
    "    Args:\n",
    "        body_part_name (str): Name of the body part (e.g., \"chest\").\n",
    "\n",
    "    Returns:\n",
    "        List[Dict]: List of exercises for the specified body part.\n",
    "    \"\"\"\n",
    "    muscle_id = muscle_name_to_id.get(body_part_name.lower())\n",
    "    if not muscle_id:\n",
    "        print(f\"No muscle ID found for body part: {body_part_name}\")\n",
    "        return []\n",
    "\n",
    "    # API request to fetch exercises\n",
    "    base_url = \"https://wger.de/api/v2/exercise/\"\n",
    "    params = {\n",
    "        \"muscles\": muscle_id,\n",
    "        \"language\": 2 # English results\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.get(base_url, params=params)\n",
    "        response.raise_for_status()\n",
    "        exercises = response.json().get(\"results\", [])\n",
    "        return exercises\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching exercises for {body_part_name}: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "aQ_BNRihfIEj"
   },
   "outputs": [],
   "source": [
    "#remove html tags\n",
    "def remove_html_tags(text):\n",
    "    clean_text = re.sub(r'<.*?>', '', text)  # Remove everything between < and >\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "FSQjLQyuAfnl"
   },
   "outputs": [],
   "source": [
    "#summarizing the description\n",
    "def summarize_text(text, max_length=None, min_length=None):\n",
    "\n",
    "    \"\"\"\n",
    "    Summarizes the input text.\n",
    "\n",
    "    Args:\n",
    "    text (str): The text to summarize.\n",
    "    max_length (int): Maximum length of the summary (optional).\n",
    "    min_length (int): Minimum length of the summary (optional).\n",
    "\n",
    "    Returns:\n",
    "    str: The summarized text.\n",
    "    \"\"\"\n",
    "\n",
    "    if not text.strip():\n",
    "        # Instead of raising ValueError, return an empty string or a message\n",
    "        return \"No description available.\" # Or \"\"\n",
    "\n",
    "    summarizer = pipeline(\"summarization\")\n",
    "    input_length = len(text.split())\n",
    "\n",
    "    # Ensure max_length and min_length have valid values\n",
    "    if max_length is None:\n",
    "        max_length = max(30, min(130, int(0.3 * input_length)))  # Default: 30% of input length, capped at 130\n",
    "    if min_length is None:\n",
    "        min_length = max(10, min(max_length - 10, int(0.1 * input_length)))  # Default: 10% of input length\n",
    "\n",
    "    # Ensure max_length is greater than min_length\n",
    "    if max_length <= min_length:\n",
    "        max_length = min_length + 10\n",
    "\n",
    "    summary = summarizer(text, max_length=max_length, min_length=min_length, do_sample=False)\n",
    "    return summary[0]['summary_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "yjxDUw9Diur1"
   },
   "outputs": [],
   "source": [
    "#Main method for the workout_plan\n",
    "def fitness_goals(text,ner):\n",
    "    text = text.lower()\n",
    "\n",
    "    final_list = []\n",
    "    body_parts = extract_body_parts(text,ner)\n",
    "    if body_parts:\n",
    "        \n",
    "        for i in body_parts:\n",
    "            final_list.append(i[0])\n",
    "        # print('Body parts identified are:', ','.join(final_list))\n",
    "        # print(\"Workout Plan Summary\".center(60, \"=\"))\n",
    "        for part in final_list:\n",
    "            exercises = get_exercises_by_body_part(part)\n",
    "            for exercise in exercises[:2]:\n",
    "                exercise_details = (f\"Exercise is {remove_html_tags(exercise['name'])} and here is its brief description: {remove_html_tags(exercise['description'])}\"\n",
    "    )\n",
    "        return exercise_details\n",
    "    else:\n",
    "        return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gE2RulwDjLzj",
    "outputId": "a3ee503a-f373-4b89-d76d-2638f551b447"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ner_body' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[42], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# #calling the main method to retrieve the work_out plans\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m fitness_goals(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMy are getting bigger from regular rows\u001b[39m\u001b[38;5;124m'\u001b[39m,ner_body)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ner_body' is not defined"
     ]
    }
   ],
   "source": [
    "# #calling the main method to retrieve the work_out plans\n",
    "fitness_goals('My are getting bigger from regular rows',ner_body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kMm9Cw24jS8N"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lW1Fjerf_lBe"
   },
   "source": [
    "# **Intent Classification**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "o_Snd8CU_kr4"
   },
   "outputs": [],
   "source": [
    "def call_bmi():\n",
    "    try:\n",
    "        # Get user input for weight and height\n",
    "        age = int(input(\"Enter your age (in years): \"))\n",
    "        weight = float(input(\"Enter your weight (in kilograms): \"))\n",
    "        height = float(input(\"Enter your height (in meters): \"))\n",
    "        \n",
    "        # Validate inputs\n",
    "        if weight <= 0 or height <= 0 or age <= 0:\n",
    "            return \"Invalid input! Age, weight, and height must be positive values.\"\n",
    "\n",
    "        # Calculate BMI\n",
    "        bmi = weight / (height ** 2)\n",
    "\n",
    "        # Categorize BMI based on standard categories\n",
    "        if bmi < 18.5:\n",
    "            category = \"Underweight\"\n",
    "        elif 18.5 <= bmi < 24.9:\n",
    "            category = \"Normal weight\"\n",
    "        elif 25 <= bmi < 29.9:\n",
    "            category = \"Overweight\"\n",
    "        else:\n",
    "            category = \"Obesity\"\n",
    "\n",
    "        # Return formatted response\n",
    "        print(f\"Your BMI is {bmi:.2f}, which falls under the category: {category}.\")\n",
    "        return f\"Your BMI is {bmi:.2f}, which falls under the category: {category}.\"\n",
    "    \n",
    "    except ValueError:\n",
    "        return \"Invalid input! Please enter numeric values for age, weight, and height.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NER FOR FOOD\n",
    "#Loading the spaCy model\n",
    "nlp_spacy_model = spacy.load(\"en_core_web_sm\")\n",
    "#Calling the method to add the food label to the Entity Ruler\n",
    "add_fooditems(nlp_spacy_model)\n",
    "ner_body = create_fitness_body_parts_ner()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JvAu3Ee2BPh5"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "## FINAL METHOD TO FOLLOW gym reccomendation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How do I stay motivated 70 25 0.127 1.5 exit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:5000\n",
      "Press CTRL+C to quit\n",
      "127.0.0.1 - - [04/Dec/2024 19:01:18] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [04/Dec/2024 19:02:47] \"POST / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [04/Dec/2024 19:05:30] \"POST / HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, request, render_template\n",
    "import pandas as pd\n",
    "import json\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import logging\n",
    "\n",
    "# Suppress Transformers warnings\n",
    "logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n",
    "\n",
    "# Initialize Flask app\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Global variable to store chat history\n",
    "chat_history = []\n",
    "\n",
    "# Load fine-tuned GPT-2 model and tokenizer\n",
    "model_path = \"C:\\\\Users\\\\ASUS\\\\Dropbox\\\\PC\\\\Downloads\\\\AIML-lab\\\\Capstone-project\\\\fitness_gpt2_model\\\\fitness_gpt2_model\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_path)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_path)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load intents and prepare training data\n",
    "with open(\"chat_data.json\", \"r\") as file:\n",
    "    intents_data = json.load(file)\n",
    "\n",
    "patterns, tags, responses = [], [], {}\n",
    "for intent in intents_data[\"intents\"]:\n",
    "    for pattern in intent[\"patterns\"]:\n",
    "        patterns.append(pattern)\n",
    "        tags.append(intent[\"tag\"])\n",
    "    responses[intent[\"tag\"]] = intent[\"responses\"]\n",
    "\n",
    "# Prepare data for intent classification\n",
    "df = pd.DataFrame({\"patterns\": patterns, \"tags\": tags})\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(df[\"patterns\"])\n",
    "encoder = LabelEncoder()\n",
    "y = encoder.fit_transform(df[\"tags\"])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a simple KNN classifier\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Preprocess user input\n",
    "def preprocess_input(input_text):\n",
    "    return input_text.strip().lower()\n",
    "\n",
    "# Predict intent with confidence scores\n",
    "def predict_intent(input_text):\n",
    "    input_vector = vectorizer.transform([input_text])\n",
    "    prediction = knn.predict(input_vector)\n",
    "    intent_tag = encoder.inverse_transform(prediction)[0]\n",
    "    confidence_scores = knn.predict_proba(input_vector)[0]\n",
    "    max_confidence = max(confidence_scores)\n",
    "    return intent_tag, max_confidence\n",
    "\n",
    "# Function to preprocess GPT-2 response\n",
    "def preprocess_response(response):\n",
    "    if \"Answer:\" in response:\n",
    "        response = response.split(\"Answer:\")[-1].strip()\n",
    "    if \"\\n\" in response:\n",
    "        response = response.split(\"\\n\")[0].strip()\n",
    "    if \".\" in response:\n",
    "        response = response.split(\".\")[0].strip() + \".\"\n",
    "    return response.strip()\n",
    "\n",
    "# Function to generate fallback response using GPT-2\n",
    "def generate_response(question, max_length=50):\n",
    "    prompt = f\"chatbot: {question}\"\n",
    "    ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "    outputs = model.generate(\n",
    "        ids,\n",
    "        do_sample=True,\n",
    "        max_length=max_length,\n",
    "        top_k=80,\n",
    "        top_p=0.9,\n",
    "        temperature=0.7,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return preprocess_response(response)\n",
    "\n",
    "# Flask Routes\n",
    "@app.route(\"/\", methods=[\"GET\", \"POST\"])\n",
    "def home():\n",
    "    global chat_history\n",
    "\n",
    "    if request.method == \"POST\":\n",
    "        user_input = request.form.get(\"user_input\", \"\")\n",
    "        user_input = preprocess_input(user_input)\n",
    "\n",
    "        # Add user input to chat history\n",
    "        chat_history.append({\"sender\": \"user\", \"message\": user_input})\n",
    "\n",
    "        # Predict intent and confidence\n",
    "        intent_tag, max_confidence = predict_intent(user_input)\n",
    "\n",
    "        if max_confidence < 0.35:  # Confidence threshold\n",
    "            gpt_response = generate_response(user_input)\n",
    "        else:\n",
    "            if intent_tag == \"a_gym_recommendation\":\n",
    "                gpt_response = \"Please provide your age, weight, and height for BMI calculation.\"\n",
    "            elif intent_tag in responses:\n",
    "                gpt_response = responses[intent_tag][0]\n",
    "            else:\n",
    "                gpt_response = generate_response(user_input)\n",
    "\n",
    "        # Add bot response to chat history\n",
    "        chat_history.append({\"sender\": \"bot\", \"message\": gpt_response})\n",
    "        return render_template(\"index.html\", messages=chat_history)\n",
    "\n",
    "    return render_template(\"index.html\", messages=chat_history)\n",
    "\n",
    "# Run Flask App\n",
    "if __name__ == \"__main__\":\n",
    "    app.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Chatbot: Hello! How can I assist you today?\n",
    "User:  hi\n",
    "Chatbot (Predefined Response): Hello! How can I assist you with your fitness goals today?\n",
    "User:  calory in beef\n",
    "Chatbot: The item is a beef. A 100g serving contains: 291.9 calories, 19.7g of total fat, including 7.8g of saturated fat, 26.6g of protein 63mg of sodium, 206mg of potassium, 87mg of cholesterol, 0.0g of total carbohydrates, with 0.0g of fiber and 0.0g of sugar\n",
    "Chatbot: Is there anything else I can help with?\n",
    "User:  i want to build biceps\n",
    "Chatbot: Exercise is Aperturas en polea and here is its brief description: Este ejercicio trabaja y tensa todo el músculo del pecho. Este entrenamiento de pecho es bueno, pero el equipo generalmente solo se encuentra en gimnasios públicos.\n",
    "Chatbot: Is there anything else I can help with?\n",
    "User:  calory in beef\n",
    "Chatbot: The item is a beef. A 100g serving contains: 291.9 calories, 19.7g of total fat, including 7.8g of saturated fat, 26.6g of protein 63mg of sodium, 206mg of potassium, 87mg of cholesterol, 0.0g of total carbohydrates, with 0.0g of fiber and 0.0g of sugar\n",
    "Chatbot: Is there anything else I can help with?\n",
    "User:  How do I increase my bench press weight\n",
    "Debug: Low confidence (0.33). Sending to fine-tuned GPT-2.\n",
    "Chatbot (Fine-Tuned GPT-2 Response): Answer concisely: increase bench press weight for your online order\n",
    "User:  I need a balanced diet\n",
    "Debug: Low confidence (0.33). Sending to fine-tuned GPT-2.\n",
    "Chatbot (Fine-Tuned GPT-2 Response): Answer concisely: need balanced diet information to assist you in effectively managing your food order please adhere to the following steps 1 access the restaurants official website at websiteurl 2 locate the section labeled reviewssection 3 select the option labeled writereview 4 complete the\n",
    "User:  gym reccomendation\n",
    "Enter your age (in years):  25\n",
    "Enter your weight (in kilograms):  70\n",
    "Enter your height (in meters):  1.5\n",
    "Your BMI is 31.11, which falls under the category: Obesity.\n",
    "Chatbot (Predefined Response): Sure! Please provide your age, weight, and height to get a personalized gym recommendation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chatbot: Hello! How can I assist you today?\n",
    "User:  hi\n",
    "Chatbot (Predefined Response): Hello! How can I assist you with your fitness goals today?\n",
    "User:  I need gym advice\n",
    "Chatbot (Predefined Response): Sure! Please provide your age, weight, and height to get a personalized gym recommendation.\n",
    "User:  calory in beef\n",
    "Chatbot: The item is a beef. A 100g serving contains: 291.9 calories, 19.7g of total fat, including 7.8g of saturated fat, 26.6g of protein 63mg of sodium, 206mg of potassium, 87mg of cholesterol, 0.0g of total carbohydrates, with 0.0g of fiber and 0.0g of sugar\n",
    "Chatbot: Is there anything else I can help with?\n",
    "User:  i want to build biceps\n",
    "Chatbot: Exercise is Aperturas en polea and here is its brief description: Este ejercicio trabaja y tensa todo el músculo del pecho. Este entrenamiento de pecho es bueno, pero el equipo generalmente solo se encuentra en gimnasios públicos.\n",
    "Chatbot: Is there anything else I can help with?\n",
    "User:  I need a balanced diet\n",
    "Debug: Low confidence (0.33). Sending to fine-tuned GPT-2.\n",
    "Chatbot (Fine-Tuned GPT-2 Response): Answer concisely: need balanced diet information to assist you in effectively ordering food online\n",
    "User:  How can I improve my form\n",
    "Chatbot (Predefined Response): Hello! How can I assist you with your fitness goals today?\n",
    "User:  How do I increase my bench press weight\n",
    "Debug: Low confidence (0.33). Sending to fine-tuned GPT-2.\n",
    "Chatbot (Fine-Tuned GPT-2 Response): Answer concisely: increase bench press weight for your upcoming event at our coffee shop please adhere to the following steps 1 access our website at websiteurl 2 go to the cateringsection 3 choose the catering package that aligns with your specific needs 4 include your"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
