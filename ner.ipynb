{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "r7j5dHqk7m3v"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\asus\\anaconda3\\lib\\site-packages (4.45.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\asus\\anaconda3\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from transformers) (0.25.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from transformers) (1.24.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in c:\\users\\asus\\anaconda3\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from transformers) (0.20.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from requests->transformers) (2024.8.30)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "80ndW4rs7qt6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspellchecker\n",
      "  Downloading pyspellchecker-0.8.1-py3-none-any.whl.metadata (9.4 kB)\n",
      "Downloading pyspellchecker-0.8.1-py3-none-any.whl (6.8 MB)\n",
      "   ---------------------------------------- 0.0/6.8 MB ? eta -:--:--\n",
      "   -------------------------- ------------- 4.5/6.8 MB 24.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.8/6.8 MB 23.2 MB/s eta 0:00:00\n",
      "Installing collected packages: pyspellchecker\n",
      "Successfully installed pyspellchecker-0.8.1\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspellchecker\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "NjWQb4FE7jPd"
   },
   "outputs": [],
   "source": [
    "#import statements\n",
    "import pandas as pd\n",
    "import json\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "import requests\n",
    "from spacy.pipeline import EntityRuler\n",
    "from typing import List, Dict, Tuple\n",
    "import time\n",
    "import spacy\n",
    "from spellchecker import SpellChecker\n",
    "import re\n",
    "import logging\n",
    "\n",
    "logging.getLogger(\"transformers\").setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "65tHNoHH64gl",
    "outputId": "8a5af282-9ae9-45c1-dd54-02af04f4c599"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L6e08uHm64u7"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zfw2S8SGiTUH"
   },
   "source": [
    "# **NER for `FOOD`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "E2u1F87fBDFg"
   },
   "outputs": [],
   "source": [
    "#method to get the nutrition\n",
    "def get_calorie(items):\n",
    "    api_url = 'https://api.calorieninjas.com/v1/nutrition?query='\n",
    "    for item in items:\n",
    "        query = item\n",
    "        response = requests.get(api_url + query, headers={'X-Api-Key': '41XzPlIpAcWTDGPLoj6WAA==NcwVJXSV0o0BEeJ3'})\n",
    "        if response.status_code == requests.codes.ok:\n",
    "            return response.json()\n",
    "        else:\n",
    "            print(\"Error:\", response.status_code, response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "6DoaFF87_-DR"
   },
   "outputs": [],
   "source": [
    "#modifying the information obtained by the nutrition API in a readable format\n",
    "def print_nutritional_info(data):\n",
    "    for item in data[\"items\"]:\n",
    "        paragraph = f\"\"\"\n",
    "        The item is a {item['name']}. A 100g serving contains:\n",
    "        - {item['calories']} calories\n",
    "        - {item['fat_total_g']}g of total fat, including {item['fat_saturated_g']}g of saturated fat\n",
    "        - {item['protein_g']}g of protein\n",
    "        - {item['sodium_mg']}mg of sodium\n",
    "        - {item['potassium_mg']}mg of potassium\n",
    "        - {item['cholesterol_mg']}mg of cholesterol\n",
    "        - {item['carbohydrates_total_g']}g of total carbohydrates, with {item['fiber_g']}g of fiber and {item['sugar_g']}g of sugar\n",
    "        \"\"\"\n",
    "        print(paragraph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fmFeQoEV6_Zh"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2hSpAhWxhGMu"
   },
   "source": [
    "**NER using Entity ruler for the identification of food items**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "yxrplrzEABMX"
   },
   "outputs": [],
   "source": [
    "#Adding food label to the following items in the Entity Ruler\n",
    "def add_fooditems(nlp):\n",
    "\n",
    "    # Add custom food entities to the pipeline\n",
    "    food_items = [\n",
    "    # Meats and Fish\n",
    "    \"Chicken\", \"Mutton\", \"Pork\", \"Chicken Breast\", \"Chicken Thigh\", \"Chicken Wings\", \"Turkey Breast\",\n",
    "    \"Ground Turkey\", \"Beef\", \"Lean Beef\", \"Ground Beef\", \"Beef Steak\", \"Pork Chop\", \"Pork Tenderloin\",\n",
    "    \"Bacon\", \"Ham\", \"Salmon Fillet\", \"Tuna\", \"Tilapia\", \"Cod\", \"Shrimp\", \"Sardines\", \"Lamb Chop\",\n",
    "\n",
    "    # Eggs and Dairy\n",
    "    \"Whole Egg\", \"Egg White\", \"Whole Milk\", \"Skim Milk\", \"Greek Yogurt\", \"Regular Yogurt\", \"Cottage Cheese\",\n",
    "    \"Cheddar Cheese\", \"Mozzarella Cheese\", \"String Cheese\", \"Whipped Cream\", \"Butter\",\n",
    "\n",
    "    # Plant Based Proteins\n",
    "    \"Tofu\", \"Tempeh\", \"Seitan\", \"Black Beans\", \"Kidney Beans\", \"Chickpeas\", \"Lentils\", \"Edamame\", \"Black Eyed Peas\",\n",
    "    \"Quinoa\",\n",
    "\n",
    "    # Grains and Starches\n",
    "    \"White Rice\", \"Brown Rice\", \"Jasmine Rice\", \"White Bread\", \"Whole Wheat Bread\", \"Sourdough Bread\", \"Bagel\",\n",
    "    \"English Muffin\", \"Tortilla\", \"Pita Bread\", \"Oatmeal\", \"Steel Cut Oats\", \"Cream of Wheat\", \"Pasta\",\n",
    "    \"Whole Wheat Pasta\", \"Sweet Potato\", \"White Potato\", \"Corn\",\n",
    "\n",
    "    # Vegetables\n",
    "    \"Broccoli\", \"Spinach\", \"Kale\", \"Lettuce\", \"Cucumber\", \"Tomato\", \"Bell Pepper\", \"Carrots\", \"Green Beans\",\n",
    "    \"Asparagus\", \"Brussels Sprouts\", \"Cauliflower\", \"Zucchini\", \"Eggplant\", \"Mushrooms\", \"Onion\", \"Garlic\", \"Celery\",\n",
    "    \"Beets\", \"Artichoke\",\n",
    "\n",
    "    # Fruits\n",
    "    \"Apple\", \"Banana\", \"Orange\", \"Strawberries\", \"Blueberries\", \"Raspberries\", \"Blackberries\", \"Grapes\",\n",
    "    \"Pineapple\", \"Mango\", \"Pear\", \"Peach\", \"Plum\", \"Watermelon\", \"Cantaloupe\", \"Honeydew\", \"Kiwi\", \"Pomegranate\",\n",
    "    \"Grapefruit\", \"Lemon\", \"Lime\",\n",
    "\n",
    "    # Nuts and Seeds\n",
    "    \"Almonds\", \"Walnuts\", \"Cashews\", \"Pistachios\", \"Pecans\", \"Brazil Nuts\", \"Macadamia Nuts\", \"Peanuts\",\n",
    "    \"Chia Seeds\", \"Flax Seeds\", \"Sunflower Seeds\", \"Pumpkin Seeds\", \"Hemp Seeds\",\n",
    "\n",
    "    # Healthy Fats\n",
    "    \"Avocado\", \"Olive Oil\", \"Coconut Oil\", \"Avocado Oil\", \"Peanut Butter\", \"Almond Butter\", \"Cashew Butter\", \"Tahini\",\n",
    "\n",
    "    # Supplements\n",
    "    \"Whey Protein\", \"Casein Protein\", \"Plant Protein\", \"Mass Gainer\", \"Protein Bar\", \"Energy Bar\", \"Pre Workout\",\n",
    "    \"Creatine\", \"BCAA\",\n",
    "\n",
    "    # Processed and Packaged Foods\n",
    "    \"Protein Cookie\", \"Protein Chips\", \"Rice Cakes\", \"Crackers\", \"Granola\", \"Cereal\", \"Protein Cereal\", \"Trail Mix\",\n",
    "    \"Dried Fruit\", \"Beef Jerky\", \"Turkey Jerky\", \"Protein Shake\", \"Smoothie\",\n",
    "\n",
    "    # Condiments and Sauces\n",
    "    \"Hummus\", \"Mustard\", \"Hot Sauce\", \"Salsa\", \"Guacamole\", \"Greek Yogurt Dip\", \"Protein Spread\", \"Sugar Free Syrup\",\n",
    "    \"Low Fat Dressing\",\n",
    "\n",
    "    # Beverages\n",
    "    \"Black Coffee\", \"Green Tea\", \"Black Tea\", \"Protein Coffee\", \"Sports Drink\", \"Energy Drink\", \"Coconut Water\",\n",
    "    \"Almond Milk\", \"Soy Milk\", \"Oat Milk\", \"Protein Water\"\n",
    "]\n",
    "    ruler = nlp.add_pipe(\"entity_ruler\", before=\"ner\")\n",
    "    patterns = [{\"label\": \"FOOD\", \"pattern\": item.lower()} for item in food_items]\n",
    "    ruler.add_patterns(patterns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "olZLvpNpHBSY"
   },
   "source": [
    "**This method also follows the same context like my method** -- discuus with richard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HezzT-Q8-YxF"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NbMm0GMqhYqQ"
   },
   "source": [
    "**Main method to identify the food items and get the nutrition information**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "552HppMeBh6S"
   },
   "outputs": [],
   "source": [
    "def nutrition(nlp,text):\n",
    "    #NER identification\n",
    "    doc = nlp(text.lower())\n",
    "    items = [ent.text for ent in doc.ents if ent.label_ == \"FOOD\"]\n",
    "    if(items):\n",
    "        data = get_calorie(items)\n",
    "    elif(not items):\n",
    "        print(\"No food items found in the prompt, can you please check the question\")\n",
    "    elif data:\n",
    "        print_nutritional_info(data)\n",
    "    else:\n",
    "        print(\"Error: 'data' is None or 'items' key is missing.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "l9TDOiuphhKU"
   },
   "outputs": [],
   "source": [
    "#Loading the spaCy model\n",
    "nlp_spacy_model = spacy.load(\"en_core_web_sm\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "f_98binKhoQ3"
   },
   "outputs": [],
   "source": [
    "#Calling the method to add the food label to the Entity Ruler\n",
    "add_fooditems(nlp_spacy_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WkzLbeDDhwE3",
    "outputId": "74b60811-7033-4c0f-9bb6-1d97205434e5"
   },
   "outputs": [],
   "source": [
    "#Calling the main method\n",
    "nutrition(nlp_spacy_model,\"Tell me the calorie count of beef?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DZc2FrjBie0A"
   },
   "source": [
    "# **NER FOR `BODY_PART`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "iiB_30fyi-P3"
   },
   "outputs": [],
   "source": [
    "def fetch_workout(exercise_names):\n",
    "    \"\"\"\n",
    "    Fetch workout information for multiple exercises from Wger API\n",
    "\n",
    "    Args:\n",
    "        exercise_names (List[str]): List of exercise names to fetch\n",
    "        api_key (str): Wger API authentication key\n",
    "\n",
    "    Returns:\n",
    "        Dict: Dictionary containing workout information and summary\n",
    "    \"\"\"\n",
    "    base_url = \"https://wger.de/api/v2/exerciseinfo/\"\n",
    "    headers = {\n",
    "        \"Authorization\": \"Token 1682538c8982ab99778af60242e0f20aa9a129eb\"\n",
    "    }\n",
    "    params = {\n",
    "        \"language\": 2  # English language results\n",
    "    }\n",
    "\n",
    "    results = []\n",
    "    successful_fetches = 0\n",
    "\n",
    "    for name in exercise_names:\n",
    "        # Update params with exercise name\n",
    "        params[\"name\"] = name\n",
    "\n",
    "        try:\n",
    "            response = requests.get(\n",
    "                base_url,\n",
    "                headers=headers,\n",
    "                params=params,\n",
    "                timeout=10\n",
    "            )\n",
    "            response.raise_for_status()\n",
    "\n",
    "            data = response.json()\n",
    "            if data['results']:\n",
    "                exercise_info = data['results'][0]  # Get first matching exercise\n",
    "                results.append(exercise_info)\n",
    "                successful_fetches += 1\n",
    "\n",
    "                print(f\"\\nFound exercise: {name}\")\n",
    "                print(f\"Description: {exercise_info['description']}\")\n",
    "                print(f\"Category: {exercise_info.get('category', {}).get('name', 'N/A')}\")\n",
    "                print(f\"Equipment: {[eq.get('name', '') for eq in exercise_info.get('equipment', [])]}\")\n",
    "                print(f\"Muscles: {[m.get('name', '') for m in exercise_info.get('muscles', [])]}\")\n",
    "            else:\n",
    "                print(f\"\\nNo results found for exercise: {name}\")\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Error fetching exercise '{name}': {str(e)}\")\n",
    "\n",
    "        # Add delay between requests to avoid rate limiting\n",
    "        time.sleep(0.5)\n",
    "\n",
    "    # Create workout summary\n",
    "    workout_info = {\n",
    "        \"exercises\": results,\n",
    "        \"summary\": {\n",
    "            \"total_exercises\": len(exercise_names),\n",
    "            \"successful_fetches\": successful_fetches,\n",
    "            \"failed_fetches\": len(exercise_names) - successful_fetches,\n",
    "            \"fetch_rate\": f\"{(successful_fetches/len(exercise_names))*100:.1f}%\"\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Print summary\n",
    "    print(f\"\\nWorkout Summary:\")\n",
    "    print(f\"Successfully fetched {successful_fetches} out of {len(exercise_names)} exercises\")\n",
    "    print(f\"Success rate: {workout_info['summary']['fetch_rate']}\")\n",
    "\n",
    "    return workout_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "AeaHwyQMh0Py"
   },
   "outputs": [],
   "source": [
    "#NER MODEL using Entity Ruler method\n",
    "\n",
    "def create_fitness_body_parts_ner(patterns: List[Dict[str, str]] = None) -> spacy.language.Language:\n",
    "    \"\"\"\n",
    "    Create a custom NER pipeline for detecting fitness-related body parts.\n",
    "\n",
    "    Args:\n",
    "        patterns: Optional list of pattern dictionaries to override default patterns\n",
    "\n",
    "    Returns:\n",
    "        nlp: Configured spaCy pipeline with fitness body parts EntityRuler\n",
    "    \"\"\"\n",
    "    nlp = spacy.blank(\"en\")\n",
    "\n",
    "    if patterns is None:\n",
    "        patterns = [\n",
    "    {\"label\": \"BODY_PART\", \"pattern\": \"biceps\"},\n",
    "    {\"label\": \"BODY_PART\", \"pattern\": \"triceps\"},\n",
    "    {\"label\": \"BODY_PART\", \"pattern\": \"deltoids\"},\n",
    "    {\"label\": \"BODY_PART\", \"pattern\": \"delts\"},\n",
    "    {\"label\": \"BODY_PART\", \"pattern\": \"shoulders\"},\n",
    "    {\"label\": \"BODY_PART\", \"pattern\": \"pectorals\"},\n",
    "    {\"label\": \"BODY_PART\", \"pattern\": \"pecs\"},\n",
    "    {\"label\": \"BODY_PART\", \"pattern\": \"chest\"},\n",
    "    {\"label\": \"BODY_PART\", \"pattern\": \"upper chest\"},\n",
    "    {\"label\": \"BODY_PART\", \"pattern\": \"lower chest\"},\n",
    "    {\"label\": \"BODY_PART\", \"pattern\": \"abs\"},\n",
    "    {\"label\": \"BODY_PART\", \"pattern\": \"abdominals\"},\n",
    "    {\"label\": \"BODY_PART\", \"pattern\": \"upper abs\"},\n",
    "    {\"label\": \"BODY_PART\", \"pattern\": \"lower abs\"},\n",
    "    {\"label\": \"BODY_PART\", \"pattern\": \"obliques\"},\n",
    "    {\"label\": \"BODY_PART\", \"pattern\": \"core\"},\n",
    "    {\"label\": \"BODY_PART\", \"pattern\": \"quadriceps\"},\n",
    "    {\"label\": \"BODY_PART\", \"pattern\": \"quads\"},\n",
    "    {\"label\": \"BODY_PART\", \"pattern\": \"hamstrings\"},\n",
    "    {\"label\": \"BODY_PART\", \"pattern\": \"calves\"},\n",
    "    {\"label\": \"BODY_PART\", \"pattern\": \"glutes\"},\n",
    "    {\"label\": \"BODY_PART\", \"pattern\": \"lats\"},\n",
    "    {\"label\": \"BODY_PART\", \"pattern\": \"traps\"},\n",
    "    {\"label\": \"BODY_PART\", \"pattern\": \"back\"},\n",
    "    {\"label\": \"BODY_PART\", \"pattern\": \"upper back\"},\n",
    "    {\"label\": \"BODY_PART\", \"pattern\": \"lower back\"},\n",
    "    {\"label\": \"BODY_PART\", \"pattern\": \"mid back\"},\n",
    "    {\"label\": \"BODY_PART\", \"pattern\": \"neck\"},\n",
    "    {\"label\": \"BODY_PART\", \"pattern\": \"arms\"},\n",
    "    {\"label\": \"BODY_PART\", \"pattern\": \"forearms\"},\n",
    "    {\"label\": \"BODY_PART\", \"pattern\": \"legs\"},\n",
    "    {\"label\": \"BODY_PART\", \"pattern\": \"thighs\"},\n",
    "    {\"label\": \"BODY_PART\", \"pattern\": \"knees\"},\n",
    "    {\"label\": \"BODY_PART\", \"pattern\": \"ankles\"},\n",
    "    {\"label\": \"BODY_PART\", \"pattern\": \"upper body\"},\n",
    "    {\"label\": \"BODY_PART\", \"pattern\": \"lower body\"},\n",
    "    {\"label\": \"BODY_PART\", \"pattern\": \"midsection\"},\n",
    "    {\"label\": \"BODY_PART\", \"pattern\": \"waist\"},\n",
    "    {\"label\": \"BODY_PART\", \"pattern\": \"hips\"},\n",
    "    {\"label\": \"BODY_PART\", \"pattern\": \"shoulder\"},\n",
    "    {\"label\": \"BODY_PART\", \"pattern\": \"trapezius\"},\n",
    "    {\"label\": \"BODY_PART\", \"pattern\": \"inner thighs\"},\n",
    "    {\"label\": \"BODY_PART\", \"pattern\": \"outer thighs\"},\n",
    "    {\"label\": \"BODY_PART\", \"pattern\": \"hands\"},\n",
    "    {\"label\": \"BODY_PART\", \"pattern\": \"fingers\"},\n",
    "    {\"label\": \"BODY_PART\", \"pattern\": \"wrists\"},\n",
    "    {\"label\": \"BODY_PART\", \"pattern\": \"thumbs\"},\n",
    "    {\"label\": \"BODY_PART\", \"pattern\": \"feet\"},\n",
    "    {\"label\": \"BODY_PART\", \"pattern\": \"toes\"},\n",
    "    {\"label\": \"BODY_PART\", \"pattern\": \"elbows\"},\n",
    "    {\"label\": \"BODY_PART\", \"pattern\": \"shoulder joints\"},\n",
    "    {\"label\": \"BODY_PART\", \"pattern\": \"hip joints\"},\n",
    "    {\"label\": \"BODY_PART\", \"pattern\": \"wrist joints\"},\n",
    "    {\"label\": \"BODY_PART\", \"pattern\": \"calf muscles\"},\n",
    "    {\"label\": \"BODY_PART\", \"pattern\": \"forearm muscles\"},\n",
    "    {\"label\": \"BODY_PART\", \"pattern\": \"lower legs\"},\n",
    "    {\"label\": \"BODY_PART\", \"pattern\": \"upper legs\"},\n",
    "]\n",
    "\n",
    "\n",
    "    # Create the EntityRuler and add it to the pipeline\n",
    "    ruler = nlp.add_pipe(\"entity_ruler\", config={\"overwrite_ents\": True})\n",
    "\n",
    "    # Add the patterns to the ruler\n",
    "    ruler.add_patterns(patterns)\n",
    "\n",
    "    return nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "o-wBJWSGinXI"
   },
   "outputs": [],
   "source": [
    "#method to identify the BODY_PARTS\n",
    "def extract_body_parts(text: str, nlp: spacy.language.Language) -> List[Tuple[str, int, int]]:\n",
    "    \"\"\"\n",
    "    Extract fitness-related body parts from text.\n",
    "\n",
    "    Args:\n",
    "        text: Input text to process\n",
    "        nlp: Configured spaCy pipeline with body parts EntityRuler\n",
    "\n",
    "    Returns:\n",
    "        List of tuples containing (body_part, start_char, end_char)\n",
    "    \"\"\"\n",
    "    doc = nlp(text)\n",
    "    return [(ent.text, ent.start_char, ent.end_char) for ent in doc.ents if ent.label_ == \"BODY_PART\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "HTzh0_vaireA"
   },
   "outputs": [],
   "source": [
    "#creating the model\n",
    "ner_body = create_fitness_body_parts_ner()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "AdubtzTgi8RA"
   },
   "outputs": [],
   "source": [
    "# Initialize the spell checker\n",
    "spell = SpellChecker()\n",
    "\n",
    "def autocorrect_sentence(sentence):\n",
    "    # Tokenize the sentence into words\n",
    "    words = sentence.split()\n",
    "\n",
    "    # Correct each word\n",
    "    corrected_words = []\n",
    "    for word in words:\n",
    "        if word.lower() in spell:  # Check if the word is valid\n",
    "            corrected_words.append(word)  # Keep the word as is\n",
    "        else:\n",
    "            corrected_word = spell.correction(word)  # Suggest a correction\n",
    "            corrected_words.append(corrected_word or word)  # Use correction if available\n",
    "\n",
    "    # Join the corrected words back into a sentence\n",
    "    corrected_sentence = ' '.join(corrected_words)\n",
    "    return corrected_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "_o6ClTmzeMnH"
   },
   "outputs": [],
   "source": [
    "# Predefined mapping of body part names to muscle IDs\n",
    "muscle_name_to_id = {\n",
    "    \"chest\": 4,\n",
    "    \"biceps\": 1,\n",
    "    \"triceps\": 5,\n",
    "    \"legs\": 8,\n",
    "    \"back\": 12,\n",
    "    \"shoulders\": 2,\n",
    "    \"abs\": 6,\n",
    "}\n",
    "\n",
    "def get_exercises_by_body_part(body_part_name):\n",
    "    \"\"\"\n",
    "    Fetch exercises based on a body part name.\n",
    "\n",
    "    Args:\n",
    "        body_part_name (str): Name of the body part (e.g., \"chest\").\n",
    "\n",
    "    Returns:\n",
    "        List[Dict]: List of exercises for the specified body part.\n",
    "    \"\"\"\n",
    "    muscle_id = muscle_name_to_id.get(body_part_name.lower())\n",
    "    if not muscle_id:\n",
    "        print(f\"No muscle ID found for body part: {body_part_name}\")\n",
    "        return []\n",
    "\n",
    "    # API request to fetch exercises\n",
    "    base_url = \"https://wger.de/api/v2/exercise/\"\n",
    "    params = {\n",
    "        \"muscles\": muscle_id,\n",
    "        \"language\": 2 # English results\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.get(base_url, params=params)\n",
    "        response.raise_for_status()\n",
    "        exercises = response.json().get(\"results\", [])\n",
    "        return exercises\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching exercises for {body_part_name}: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "aQ_BNRihfIEj"
   },
   "outputs": [],
   "source": [
    "#remove html tags\n",
    "def remove_html_tags(text):\n",
    "    clean_text = re.sub(r'<.*?>', '', text)  # Remove everything between < and >\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "FSQjLQyuAfnl"
   },
   "outputs": [],
   "source": [
    "#summarizing the description\n",
    "def summarize_text(text, max_length=None, min_length=None):\n",
    "\n",
    "    \"\"\"\n",
    "    Summarizes the input text.\n",
    "\n",
    "    Args:\n",
    "    text (str): The text to summarize.\n",
    "    max_length (int): Maximum length of the summary (optional).\n",
    "    min_length (int): Minimum length of the summary (optional).\n",
    "\n",
    "    Returns:\n",
    "    str: The summarized text.\n",
    "    \"\"\"\n",
    "\n",
    "    if not text.strip():\n",
    "        # Instead of raising ValueError, return an empty string or a message\n",
    "        return \"No description available.\" # Or \"\"\n",
    "\n",
    "    summarizer = pipeline(\"summarization\")\n",
    "    input_length = len(text.split())\n",
    "\n",
    "    # Ensure max_length and min_length have valid values\n",
    "    if max_length is None:\n",
    "        max_length = max(30, min(130, int(0.3 * input_length)))  # Default: 30% of input length, capped at 130\n",
    "    if min_length is None:\n",
    "        min_length = max(10, min(max_length - 10, int(0.1 * input_length)))  # Default: 10% of input length\n",
    "\n",
    "    # Ensure max_length is greater than min_length\n",
    "    if max_length <= min_length:\n",
    "        max_length = min_length + 10\n",
    "\n",
    "    summary = summarizer(text, max_length=max_length, min_length=min_length, do_sample=False)\n",
    "    return summary[0]['summary_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "yjxDUw9Diur1"
   },
   "outputs": [],
   "source": [
    "#Main method for the workout_plan\n",
    "def fitness_goals(text,ner):\n",
    "    text = text.lower()\n",
    "\n",
    "    text = autocorrect_sentence(text)\n",
    "\n",
    "    final_list = []\n",
    "    body_parts = extract_body_parts(text,ner)\n",
    "    print(len(body_parts))\n",
    "    for i in body_parts:\n",
    "        final_list.append(i[0])\n",
    "    print('Body parts identified are:', ','.join(final_list))\n",
    "    print(\"Workout Plan Summary\".center(60, \"=\"))\n",
    "    for part in final_list:\n",
    "        exercises = get_exercises_by_body_part(part)\n",
    "        for exercise in exercises:\n",
    "            print(f\"\\nExercise: {remove_html_tags(exercise['name'])}\")\n",
    "            print(f\"Description: {remove_html_tags(exercise['description'])}\")\n",
    "            print(\"-\" * 60)\n",
    "    fetch_workout(final_list) #API CALL --> not getting the response from the API\n",
    "\n",
    "    SUMMARIZE\n",
    "    summarize_text()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gE2RulwDjLzj",
    "outputId": "a3ee503a-f373-4b89-d76d-2638f551b447"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Body parts identified are: abs\n",
      "====================Workout Plan Summary====================\n",
      "\n",
      "Exercise: Abdominal Stabilization\n",
      "Description: \n",
      "------------------------------------------------------------\n",
      "\n",
      "Exercise: Abdominales sovieticas\n",
      "Description: sentado en el suelo agarramos la pesa rusa, flexionamos las piernas y las separamos del suelo. Ahora llevaremos a un lado y otro la pesa\n",
      "------------------------------------------------------------\n",
      "\n",
      "Exercise: Bear Walk\n",
      "Description: -Rest your weight on your palms and the balls of your feet, not dissimilar to normal pushup position\n",
      "-Move by stepping with your R palm and L foot, then your L palm and R foot.  Basically, walk like a lumbering bear.\n",
      "-Move as fast as you can.  Measure your reps/sets in either distance (i.e. 40 yards) or time (i.e. 45 seconds)\n",
      "-Works your Pecs, Deltoids, Triceps, Traps, Lats, Abs and Lower Back, Hip Flexors, Quads, Glutes and Calves\n",
      "------------------------------------------------------------\n",
      "\n",
      "Exercise: Crunches\n",
      "Description: Lay down on your back a soft surface, the feet are on the floor. Ask a partner or use some other help (barbell, etc.) to keep them fixed, your hands are behind your head. From this position move your upper body up till your head or elbows touch your knees. Do this movement by rolling up your back.\n",
      "------------------------------------------------------------\n",
      "\n",
      "Exercise: Crunches HD\n",
      "Description: Crunch movement is one of the most basic exercises designed to strengthen the core muscles of the body. Exercise helps to strengthen core muscles, improve posture, and increase muscle mobility and flexibility.\n",
      "Improves six pack muscles: When crunch exercise is done, the rectus abdominus and oblique muscles are tightened, so the upper abdominal muscles and six pack muscles develop.\n",
      "Increases the strength of the abdominal muscles: The primary role of your abdominal muscles is to stabilize your mid-section. It supports you while lifting heavy objects, allowing you to twist and rotate your body. These are all day long actions that you do not notice, so it is important that your abdominal muscles can sustain long hours of work. Crunch exercise helps build this important endurance in the abdominal muscles. Muscular endurance is the ability of these fibers to resist resistance for a long time.\n",
      "------------------------------------------------------------\n",
      "\n",
      "Exercise: Crunches With Cable\n",
      "Description: Take the cable on your hands and hold it next to your temples. Knee down and hold your upper body straight and bend forward. Go down with a fast movement, rolling your back in (your ellbows point to your knees). Once down, go slowly back to the initial position.\n",
      "------------------------------------------------------------\n",
      "\n",
      "Exercise: Crunches With Legs Up\n",
      "Description: On your back, legs extended straight up, reach toward your toes with your hands and lift your shoulder blades off the ground and back.\n",
      "------------------------------------------------------------\n",
      "\n",
      "Exercise: Crunches on Machine\n",
      "Description: The procedure is very similar as for regular crunches, only with the additional weight of the machine. Sit on the machine, put both feet firmly on the ground. Grab the to the weights, cables, etc. and do a rolling motion forwards (the spine should ideally lose touch vertebra by vertebra). Slowly return to the starting position. \n",
      "------------------------------------------------------------\n",
      "\n",
      "Exercise: Crunches on incline bench\n",
      "Description: Lay down on your back on a inclined bench, feet are on one end of the bench. Ask a partner or use some other help (barbell, etc.) to keep them fixed, your hands are behind your head. From this position move your upper body up till your head or elbows touch your knees. Do this movement by rolling up your back.\n",
      "------------------------------------------------------------\n",
      "\n",
      "Exercise: Flutter Kicks\n",
      "Description: -Laying on the back, lift your straightened legs from the ground at a 45 degree angle. \n",
      "-As your Left foot travels downward and nearly touches the floor, your Right foot should seek to reach a 90 degree angle, or as close to one as possible.\n",
      "-Bring your R foot down until it nearly touches the floor, and bring your L foot upwards.  Maintain leg rigidity throughout the exercise.  Your head should stay off the ground, supported by tightened upper abdominals.\n",
      "-(L up R down, L down R up, x2)  ^v, v^, ^v, v^ = 1 rep\n",
      "-Primarily works the Rectus Abdominus, the hip flexors and the lower back. Secondarily works the Obliques.  Emphasis placed on the lower quadrant of the abs.\n",
      " \n",
      "------------------------------------------------------------\n",
      "\n",
      "Exercise: Frog stand\n",
      "Description: Starting position:\n",
      "\n",
      "Stand with your feet shoulder-width apart and your toes pointing forward, facing a wall or bench for support if needed.\n",
      "Bend your knees into a squat position.\n",
      "Place your hands on the ground in front of you, shoulder-width apart, with your fingers spread wide.\n",
      "Make sure your elbows are under your shoulders and your body forms a straight line from your head to your heels.\n",
      "Upward phase:\n",
      "\n",
      "Push with your feet and hands to lift your body off the ground.\n",
      "Straighten your legs and arms, keeping your body aligned.\n",
      "Engage your core and glutes to maintain the position.\n",
      "If needed, rest your knees on the wall or bench for assistance.\n",
      "------------------------------------------------------------\n",
      "\n",
      "Exercise: Front Lever\n",
      "Description: The front lever is a figure where the body is kept in a horizontal position parallel to the floor.\n",
      "------------------------------------------------------------\n",
      "\n",
      "Exercise: Front Plank\n",
      "Description: The plank is a bodyweight exercise. As a multi-functional movement, the plank not only targets your abdominal muscles but also the spine and hip. Plank strengthens and tightens your entire body, improves your posture and balance, reduces body fat, and can help boost your metabolism.\n",
      "Exercises such as the “plank pose” help strengthen the stamina of stabilizing abdominal muscles. It can also help relieve back pain associated with a weakening of the function of the stabilizing muscles of the body.\n",
      "Planks are a versatile exercise that targets many of the most important muscle groups in the body, so they can be applied by anyone to improve endurance and overall body strength.\n",
      "------------------------------------------------------------\n",
      "\n",
      "Exercise: Front lever pull-up\n",
      "Description: in the front lever position, with legs extended or easier if collected, pull by bringing the chest closer to the bar.\n",
      "------------------------------------------------------------\n",
      "\n",
      "Exercise: Full Sit Outs\n",
      "Description: (A) Get in high plank position on your hands and toes.(B) Shift your weight to your left hand as you turn your body to the right; bend your right leg behind you and extend your right arm up. Return to the center and repeat on the opposite side. Continue, alternating sides.Make it easier: Don’t raise your arm after you bend your leg behind you.Make it harder: Balance with your arm and leg extended for two counts.\n",
      "------------------------------------------------------------\n",
      "\n",
      "Exercise: Hanging Leg Raises\n",
      "Description: Hanging from bar or straps, bring legs up with knees extended or flexed\n",
      "------------------------------------------------------------\n",
      "\n",
      "Exercise: High Knee Skips HD\n",
      "Description: You can use this exercise both as a dynamic warm-up before training and add it to your cardio training routine to burn fat.\n",
      "\n",
      "High knee skips are a plyometric exercise. Plyometrics are explosive aerobic moves that increase speed, quickness, and power and they work your whole body.\n",
      "\n",
      "High knee skips target the oblique, leg muscles, hip muscles and hip flexors and also work on the thighs, knee tendons, quadriceps and shoulders.\n",
      "\n",
      "Jumps are beneficial to your health because they combine cardiovascular conditioning with strength work. Since jumps elevate your heart rate, they can also improve your cardiovascular fitness.\n",
      "------------------------------------------------------------\n",
      "\n",
      "Exercise: Hollow Hold\n",
      "Description: Get on a mat and lie on your back. Contract your abs, stretch your raise and legs and raise them (your head and shoulders are also be raised). Make sure your lower back remains in contact with the mat.\n",
      "------------------------------------------------------------\n",
      "\n",
      "Exercise: Incline Plank With Alternate Floor Touch\n",
      "Description: Perform the plank with legs elevated, feet on a gymball. Once stabilised, slowly move one foot sideways off the ball, then make it touch the floor, then come back to starting position. Alternate with the other foot.\n",
      "This is a core exercise.\n",
      "------------------------------------------------------------\n",
      "\n",
      "Exercise: Kopfüber Gewichtaufheben\n",
      "Description: Diese Maschine neben Beinheben.\n",
      "Kopf nach vorne, mit dem gesicht zum boden und das Gewicht aufheben\n",
      "------------------------------------------------------------\n",
      "\n",
      "No results found for exercise: abs\n",
      "\n",
      "Workout Summary:\n",
      "Successfully fetched 0 out of 1 exercises\n",
      "Success rate: 0.0%\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'SUMMARIZE' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[58], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#calling the main method to retrieve the work_out plans\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m fitness_goals(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMy abs are getting bigger from regular rows\u001b[39m\u001b[38;5;124m'\u001b[39m,ner_body)\n",
      "Cell \u001b[1;32mIn[56], line 22\u001b[0m, in \u001b[0;36mfitness_goals\u001b[1;34m(text, ner)\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m60\u001b[39m)\n\u001b[0;32m     20\u001b[0m fetch_workout(final_list) \u001b[38;5;66;03m#API CALL --> not getting the response from the API\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m SUMMARIZE\n\u001b[0;32m     23\u001b[0m summarize_text()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'SUMMARIZE' is not defined"
     ]
    }
   ],
   "source": [
    "#calling the main method to retrieve the work_out plans\n",
    "fitness_goals('My abs are getting bigger from regular rows',ner_body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kMm9Cw24jS8N"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lW1Fjerf_lBe"
   },
   "source": [
    "# **Intent Classification**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o_Snd8CU_kr4"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XiaWYWlN_cgs",
    "outputId": "5b167893-4b14-41c1-bcd3-0eec47e4ac01"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intent: good_bye_intent\n",
      "Response: Have a good day\n"
     ]
    }
   ],
   "source": [
    "# Example Interaction\n",
    "user_input = \"Tell me the calorie count of beef?\"\n",
    "\n",
    "# Step 1: Predict Intent\n",
    "intent_tag, response = predict_intent(user_input)\n",
    "\n",
    "# Step 2: Handle Specific Intents\n",
    "if intent_tag == \"gym_recommendation\":  # Assuming 'gym_recommendation' is the tag for BMI-based recommendations\n",
    "    print(\"Bot:\", response)  # Ask for age, weight, height\n",
    "elif intent_tag == \"provide_gym_plan\":  # Handle user input for age, weight, and height\n",
    "    # Parse user input for age, weight, and height\n",
    "    age, weight, height = parse_user_input(user_input)\n",
    "    if age and weight and height:\n",
    "        bmi_recommendation = gym_recommendation(age, weight, height)\n",
    "        print(f\"Intent: {intent_tag}\")\n",
    "        print(f\"Response: {bmi_recommendation}\")\n",
    "    else:\n",
    "        print(\"Bot: I couldn't understand your age, weight, and height. Could you provide them again?\")\n",
    "\n",
    "elif intent_tag == \"workout_plan\":  # Handle Workout plans\n",
    "    ner_body = create_fitness_body_parts_ner()\n",
    "    # print('Can you please specify any part of the muscle you want to focus on')\n",
    "    # Will take the input from the user for the body_parts or change the patterns in the json\n",
    "    # Calling the main method to retrieve the workout plans\n",
    "    fitness_goals(user_input, ner_body)\n",
    "\n",
    "elif intent_tag == \"nutrition_plan\":  # Handle Calorie information\n",
    "    # Loading the spaCy model\n",
    "    nlp_spacy_model = spacy.load(\"en_core_web_sm\")\n",
    "    # Calling the method to add the food label to the Entity Ruler\n",
    "    add_fooditems(nlp_spacy_model)\n",
    "    # Calling the main method\n",
    "    nutrition(nlp_spacy_model, user_input)\n",
    "\n",
    "else:\n",
    "    print(f\"Intent: {intent_tag}\")\n",
    "    print(f\"Response: {response}\")\n",
    "\n",
    "# Function Definitions (e.g., `nutrition`, `add_fooditems`, `fitness_goals`) should follow here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NER model saved to 'model/ner_model.pkl'\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# Load a pre-trained spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Add a new NER pipeline if needed\n",
    "ner = nlp.get_pipe(\"ner\")\n",
    "\n",
    "# (Optional) Add labels and training here...\n",
    "\n",
    "# Ensure the 'model' directory exists\n",
    "os.makedirs('model', exist_ok=True)\n",
    "\n",
    "# Save the trained NER model to a .pkl file\n",
    "with open('model/ner_model.pkl', 'wb') as file:\n",
    "    pickle.dump(ner, file)\n",
    "\n",
    "print(\"NER model saved to 'model/ner_model.pkl'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NER model saved to 'model/ner_model.pkl'\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# Load pre-trained NER from spaCy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "ner = nlp.get_pipe(\"ner\")\n",
    "\n",
    "# Ensure the 'model' directory exists\n",
    "os.makedirs('model', exist_ok=True)\n",
    "\n",
    "# Save the NER model to a .pkl file\n",
    "with open('model/ner_model.pkl', 'wb') as file:\n",
    "    pickle.dump(ner, file)\n",
    "\n",
    "print(\"NER model saved to 'model/ner_model.pkl'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "id": "JvAu3Ee2BPh5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface_hub in c:\\users\\asus\\anaconda3\\lib\\site-packages (0.25.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\asus\\anaconda3\\lib\\site-packages (from huggingface_hub) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from huggingface_hub) (2023.10.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from huggingface_hub) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from huggingface_hub) (6.0.1)\n",
      "Requirement already satisfied: requests in c:\\users\\asus\\anaconda3\\lib\\site-packages (from huggingface_hub) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from huggingface_hub) (4.65.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from huggingface_hub) (4.12.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tqdm>=4.42.1->huggingface_hub) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from requests->huggingface_hub) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from requests->huggingface_hub) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from requests->huggingface_hub) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from requests->huggingface_hub) (2024.8.30)\n"
     ]
    }
   ],
   "source": [
    "!pip install huggingface_hub\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:5000\n",
      "Press CTRL+C to quit\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, request, render_template\n",
    "import pandas as pd\n",
    "import json\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import logging\n",
    "\n",
    "# Suppress Transformers warnings\n",
    "logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n",
    "\n",
    "# Initialize Flask app\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Load GPT-2 model and tokenizer\n",
    "model_path = \"gpt2\"  # Replace with your GPT-2 model path if needed\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_path)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_path)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load intents and prepare training data\n",
    "with open(\"chat_data.json\", \"r\") as file:\n",
    "    intents_data = json.load(file)\n",
    "\n",
    "patterns, tags, responses = [], [], {}\n",
    "for intent in intents_data[\"intents\"]:\n",
    "    for pattern in intent[\"patterns\"]:\n",
    "        patterns.append(pattern)\n",
    "        tags.append(intent[\"tag\"])\n",
    "    responses[intent[\"tag\"]] = intent[\"responses\"]\n",
    "\n",
    "# Prepare data for intent classification\n",
    "df = pd.DataFrame({\"patterns\": patterns, \"tags\": tags})\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(df[\"patterns\"])\n",
    "encoder = LabelEncoder()\n",
    "y = encoder.fit_transform(df[\"tags\"])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a Random Forest classifier\n",
    "classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Function to generate GPT-2 fallback response\n",
    "def generate_gpt2_response(question, max_length=50):\n",
    "    prompt = f\"Answer this query: {question}\"\n",
    "    ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "    outputs = model.generate(\n",
    "        ids,\n",
    "        do_sample=True,\n",
    "        max_length=max_length,\n",
    "        top_k=50,\n",
    "        top_p=0.9,\n",
    "        temperature=0.7,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
    "    return response\n",
    "\n",
    "# Predict intent with confidence\n",
    "def predict_intent(user_input):\n",
    "    input_vector = vectorizer.transform([user_input])\n",
    "    prediction = classifier.predict(input_vector)\n",
    "    intent_tag = encoder.inverse_transform(prediction)[0]\n",
    "\n",
    "    # Calculate confidence\n",
    "    confidence_scores = classifier.predict_proba(input_vector)[0]\n",
    "    max_confidence = max(confidence_scores)\n",
    "    return intent_tag, max_confidence\n",
    "\n",
    "# Flask Routes\n",
    "@app.route(\"/\", methods=[\"GET\", \"POST\"])\n",
    "def home():\n",
    "    if request.method == \"POST\":\n",
    "        user_input = request.form.get(\"user_input\", \"\").strip()\n",
    "\n",
    "        # Predict intent and confidence\n",
    "        intent_tag, max_confidence = predict_intent(user_input)\n",
    "\n",
    "        if max_confidence > 0.5:  # Confidence threshold for intent classification\n",
    "            predefined_response = responses.get(intent_tag, [\"Sorry, I don't have an answer for that.\"])[0]\n",
    "            response_type = f\"Intent Response ({intent_tag})\"\n",
    "        else:\n",
    "            predefined_response = generate_gpt2_response(user_input)\n",
    "            response_type = \"Chatbot Response\"\n",
    "\n",
    "        return render_template(\n",
    "            \"index.html\",\n",
    "            user_input=user_input,\n",
    "            response=predefined_response,\n",
    "            response_type=response_type,\n",
    "        )\n",
    "    return render_template(\"index.html\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:5000\n",
      "Press CTRL+C to quit\n",
      "127.0.0.1 - - [30/Nov/2024 23:19:39] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [30/Nov/2024 23:19:52] \"POST / HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, request, render_template\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import json\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Initialize Flask app\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Load GPT-2 model and tokenizer\n",
    "model_path = \"gpt2\"  # Replace with your GPT-2 model path if needed\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_path)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_path)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load intents and prepare training data\n",
    "with open(\"chat_data.json\", \"r\") as file:\n",
    "    intents_data = json.load(file)\n",
    "\n",
    "patterns, tags, responses = [], [], {}\n",
    "for intent in intents_data[\"intents\"]:\n",
    "    for pattern in intent[\"patterns\"]:\n",
    "        patterns.append(pattern)\n",
    "        tags.append(intent[\"tag\"])\n",
    "    responses[intent[\"tag\"]] = intent[\"responses\"]\n",
    "\n",
    "# Prepare data for intent classification\n",
    "df = pd.DataFrame({\"patterns\": patterns, \"tags\": tags})\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(df[\"patterns\"])\n",
    "encoder = LabelEncoder()\n",
    "y = encoder.fit_transform(df[\"tags\"])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a Random Forest classifier\n",
    "classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Function to generate GPT-2 fallback response\n",
    "def generate_gpt2_response(question, max_length=50):\n",
    "    prompt = f\": {question}\"\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "    outputs = model.generate(\n",
    "        input_ids,\n",
    "        max_length=max_length,\n",
    "        do_sample=True,\n",
    "        top_k=50,\n",
    "        top_p=0.9,\n",
    "        temperature=0.7,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
    "    return response\n",
    "\n",
    "# Predict intent with confidence\n",
    "def predict_intent(user_input):\n",
    "    input_vector = vectorizer.transform([user_input])\n",
    "    prediction = classifier.predict(input_vector)\n",
    "    intent_tag = encoder.inverse_transform(prediction)[0]\n",
    "    confidence_scores = classifier.predict_proba(input_vector)[0]\n",
    "    max_confidence = max(confidence_scores)\n",
    "    return intent_tag, max_confidence\n",
    "\n",
    "# Flask route for handling requests\n",
    "@app.route(\"/\", methods=[\"GET\", \"POST\"])\n",
    "def home():\n",
    "    if request.method == \"POST\":\n",
    "        user_input = request.form.get(\"user_input\", \"\").strip()\n",
    "        intent_tag, max_confidence = predict_intent(user_input)\n",
    "\n",
    "        if max_confidence > 0.5:  # Confidence threshold\n",
    "            response = responses.get(intent_tag, [\"Sorry, I don't have an answer for that.\"])[0]\n",
    "        else:\n",
    "            response = generate_gpt2_response(user_input)\n",
    "\n",
    "        return render_template(\"index.html\", user_input=user_input, response=response)\n",
    "    return render_template(\"index.html\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:5000\n",
      "Press CTRL+C to quit\n",
      "127.0.0.1 - - [30/Nov/2024 23:44:08] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [30/Nov/2024 23:44:12] \"POST / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [30/Nov/2024 23:44:17] \"POST / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [30/Nov/2024 23:44:30] \"POST / HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, request, render_template\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import pandas as pd\n",
    "import json\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Initialize Flask app\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Load fine-tuned GPT-2 model and tokenizer\n",
    "model_path = \"C:\\\\Users\\\\ASUS\\\\Dropbox\\\\PC\\\\Downloads\\\\AIML-lab\\\\Capstone-project\\\\gpt2_fine_tuned_model\\\\gpt2_fine_tuned_model\"  # Replace with the directory containing your uploaded model files\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_path)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_path)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load intents and prepare training data\n",
    "with open(\"chat_data.json\", \"r\") as file:\n",
    "    intents_data = json.load(file)\n",
    "\n",
    "patterns, tags, responses = [], [], {}\n",
    "for intent in intents_data[\"intents\"]:\n",
    "    for pattern in intent[\"patterns\"]:\n",
    "        patterns.append(pattern)\n",
    "        tags.append(intent[\"tag\"])\n",
    "    responses[intent[\"tag\"]] = intent[\"responses\"]\n",
    "\n",
    "# Prepare data for intent classification\n",
    "df = pd.DataFrame({\"patterns\": patterns, \"tags\": tags})\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(df[\"patterns\"])\n",
    "encoder = LabelEncoder()\n",
    "y = encoder.fit_transform(df[\"tags\"])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a Random Forest classifier\n",
    "classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Function to generate GPT-2 fallback response\n",
    "def generate_gpt2_response(question, max_length=50):\n",
    "    prompt = f\": {question}\"\n",
    "    ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "    outputs = model.generate(\n",
    "        ids,\n",
    "        do_sample=True,\n",
    "        max_length=max_length,\n",
    "        top_k=50,\n",
    "        top_p=0.9,\n",
    "        temperature=0.7,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
    "    return response\n",
    "\n",
    "# Predict intent with confidence\n",
    "def predict_intent(user_input):\n",
    "    input_vector = vectorizer.transform([user_input])\n",
    "    prediction = classifier.predict(input_vector)\n",
    "    intent_tag = encoder.inverse_transform(prediction)[0]\n",
    "\n",
    "    # Calculate confidence\n",
    "    confidence_scores = classifier.predict_proba(input_vector)[0]\n",
    "    max_confidence = max(confidence_scores)\n",
    "    return intent_tag, max_confidence\n",
    "\n",
    "# Flask Routes\n",
    "@app.route(\"/\", methods=[\"GET\", \"POST\"])\n",
    "def home():\n",
    "    if request.method == \"POST\":\n",
    "        user_input = request.form.get(\"user_input\", \"\").strip()\n",
    "\n",
    "        # Predict intent and confidence\n",
    "        intent_tag, max_confidence = predict_intent(user_input)\n",
    "\n",
    "        if max_confidence > 0.5:  # Confidence threshold for intent classification\n",
    "            response = responses.get(intent_tag, [\"I'm not sure how to help with that.\"])[0]\n",
    "        else:\n",
    "            response = generate_gpt2_response(user_input)\n",
    "\n",
    "        return render_template(\n",
    "            \"index.html\",\n",
    "            user_input=user_input,\n",
    "            response=response,\n",
    "        )\n",
    "    return render_template(\"index.html\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:5000\n",
      "Press CTRL+C to quit\n",
      "127.0.0.1 - - [30/Nov/2024 23:45:02] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [30/Nov/2024 23:45:06] \"POST / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [30/Nov/2024 23:45:17] \"POST / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [30/Nov/2024 23:46:50] \"POST / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [30/Nov/2024 23:46:59] \"POST / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [30/Nov/2024 23:47:19] \"POST / HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, request, render_template\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import pandas as pd\n",
    "import json\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Initialize Flask app\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Load fine-tuned GPT-2 model and tokenizer\n",
    "model_path = \"C:\\\\Users\\\\ASUS\\Dropbox\\\\PC\\Downloads\\\\AIML-lab\\\\Capstone-project\\\\fitness_gpt2_model\\\\fitness_gpt2_model\"  # Replace with the directory containing your uploaded model files\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_path)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_path)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load intents and prepare training data\n",
    "with open(\"chat_data.json\", \"r\") as file:\n",
    "    intents_data = json.load(file)\n",
    "\n",
    "patterns, tags, responses = [], [], {}\n",
    "for intent in intents_data[\"intents\"]:\n",
    "    for pattern in intent[\"patterns\"]:\n",
    "        patterns.append(pattern)\n",
    "        tags.append(intent[\"tag\"])\n",
    "    responses[intent[\"tag\"]] = intent[\"responses\"]\n",
    "\n",
    "# Prepare data for intent classification\n",
    "df = pd.DataFrame({\"patterns\": patterns, \"tags\": tags})\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(df[\"patterns\"])\n",
    "encoder = LabelEncoder()\n",
    "y = encoder.fit_transform(df[\"tags\"])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a Random Forest classifier\n",
    "classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Function to generate GPT-2 fallback response\n",
    "def generate_gpt2_response(question, max_length=50):\n",
    "    prompt = f\": {question}\"\n",
    "    ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "    outputs = model.generate(\n",
    "        ids,\n",
    "        do_sample=True,\n",
    "        max_length=max_length,\n",
    "        top_k=50,\n",
    "        top_p=0.9,\n",
    "        temperature=0.7,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
    "    return response\n",
    "\n",
    "# Predict intent with confidence\n",
    "def predict_intent(user_input):\n",
    "    input_vector = vectorizer.transform([user_input])\n",
    "    prediction = classifier.predict(input_vector)\n",
    "    intent_tag = encoder.inverse_transform(prediction)[0]\n",
    "\n",
    "    # Calculate confidence\n",
    "    confidence_scores = classifier.predict_proba(input_vector)[0]\n",
    "    max_confidence = max(confidence_scores)\n",
    "    return intent_tag, max_confidence\n",
    "\n",
    "# Flask Routes\n",
    "@app.route(\"/\", methods=[\"GET\", \"POST\"])\n",
    "def home():\n",
    "    if request.method == \"POST\":\n",
    "        user_input = request.form.get(\"user_input\", \"\").strip()\n",
    "\n",
    "        # Predict intent and confidence\n",
    "        intent_tag, max_confidence = predict_intent(user_input)\n",
    "\n",
    "        if max_confidence > 0.5:  # Confidence threshold for intent classification\n",
    "            response = responses.get(intent_tag, [\"I'm not sure how to help with that.\"])[0]\n",
    "        else:\n",
    "            response = generate_gpt2_response(user_input)\n",
    "\n",
    "        return render_template(\n",
    "            \"index.html\",\n",
    "            user_input=user_input,\n",
    "            response=response,\n",
    "        )\n",
    "    return render_template(\"index.html\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
